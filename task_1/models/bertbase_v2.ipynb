{"cells":[{"cell_type":"code","execution_count":56,"id":"6b7b8fab-d389-4934-a6b4-5da2929bf008","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3897,"status":"ok","timestamp":1651239567115,"user":{"displayName":"Weimin Ouyang","userId":"09960439250658389642"},"user_tz":-600},"id":"6b7b8fab-d389-4934-a6b4-5da2929bf008","outputId":"fecb2da3-4c2b-4a48-b0b4-7525babd17f8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.12.0+cu113)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"]}],"source":["# !pip install torch torchvision transformers"]},{"cell_type":"code","execution_count":57,"id":"cdeab1e4-7836-49c1-8141-2daeadd69d8a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1651239567115,"user":{"displayName":"Weimin Ouyang","userId":"09960439250658389642"},"user_tz":-600},"id":"cdeab1e4-7836-49c1-8141-2daeadd69d8a","outputId":"0f7db4e0-275b-4b5a-c600-27e490b0de0b"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import re\n","import string\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from nltk.tokenize import TweetTokenizer\n","import torch\n","from transformers import BertTokenizer, get_linear_schedule_with_warmup\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import torch\n","import torch.nn as nn\n","from transformers import BertModel"]},{"cell_type":"code","execution_count":58,"id":"137181cc-8159-424b-945f-04dbe37bc50b","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1651239567116,"user":{"displayName":"Weimin Ouyang","userId":"09960439250658389642"},"user_tz":-600},"id":"137181cc-8159-424b-945f-04dbe37bc50b"},"outputs":[],"source":["train = pd.read_csv('/content/sample_data/train_label.csv')\n","dev = pd.read_csv('/content/sample_data/dev_label.csv')\n","test = pd.read_csv('/content/sample_data/test.csv')"]},{"cell_type":"code","execution_count":59,"id":"0878e433-caf2-4b13-86f8-f71e3b6736ee","metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1651239567721,"user":{"displayName":"Weimin Ouyang","userId":"09960439250658389642"},"user_tz":-600},"id":"0878e433-caf2-4b13-86f8-f71e3b6736ee"},"outputs":[],"source":["train = train[['text', 'label']]\n","dev = dev[['text', 'label']]\n","test = test[['text']]"]},{"cell_type":"code","execution_count":60,"id":"BRnVHG3PeLPo","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1651239569450,"user":{"displayName":"Weimin Ouyang","userId":"09960439250658389642"},"user_tz":-600},"id":"BRnVHG3PeLPo"},"outputs":[],"source":["def process_tweet(tweet):\n","    \n","    stopwords_english = stopwords.words('english')\n","    # remove stock market tickers like $GE\n","    tweet = re.sub(r'\\$\\w*', '', tweet)\n","    # remove old style retweet text \"RT\"\n","    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n","    # remove hyperlinks\n","    tweet = re.sub(r'http\\S+', '', tweet)\n","    # remove hashtags\n","    # only removing the hash # sign from the word\n","    tweet = re.sub(r'#', '', tweet)\n","    # tokenize tweets\n","    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n","                               reduce_len=True)\n","    tweet_tokens = tokenizer.tokenize(tweet)\n","\n","    tweets_clean = []\n","    for word in tweet_tokens:\n","        if (word not in stopwords_english and  # remove stopwords\n","                word not in string.punctuation):  # remove punctuation\n","            tweets_clean.append(word)\n","            \n","    tweet_clean = ' '.join(tweets_clean)\n","\n","    return tweet_clean"]},{"cell_type":"code","execution_count":61,"id":"xFp0xTdteNiw","metadata":{"executionInfo":{"elapsed":5000,"status":"ok","timestamp":1651239576081,"user":{"displayName":"Weimin Ouyang","userId":"09960439250658389642"},"user_tz":-600},"id":"xFp0xTdteNiw"},"outputs":[],"source":["processed_train = [process_tweet(t) for t in train.text]\n","processed_dev = [process_tweet(t) for t in dev.text]\n","processed_test = [process_tweet(t) for t in test.text]"]},{"cell_type":"code","execution_count":62,"id":"8e688828-a257-49c4-9a6b-07982efb5aaa","metadata":{"executionInfo":{"elapsed":739,"status":"ok","timestamp":1651239576816,"user":{"displayName":"Weimin Ouyang","userId":"09960439250658389642"},"user_tz":-600},"id":"8e688828-a257-49c4-9a6b-07982efb5aaa"},"outputs":[],"source":["train.to_csv('train_text.csv', index = False)\n","dev.to_csv('dev_text.csv', index = False)\n","test.to_csv('test_text.csv', index = False)"]},{"cell_type":"code","execution_count":63,"id":"d9d3869b-3070-4e70-a253-3efde00b1ffb","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1651239577866,"user":{"displayName":"Weimin Ouyang","userId":"09960439250658389642"},"user_tz":-600},"id":"d9d3869b-3070-4e70-a253-3efde00b1ffb"},"outputs":[],"source":["class SSTDataset(Dataset):\n","\n","    def __init__(self, filename, maxlen):\n","\n","        #Store the contents of the file in a pandas dataframe\n","        self.df = pd.read_csv(filename)\n","\n","        #Initialize the BERT tokenizer\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","        self.maxlen = maxlen\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index):\n","\n","        #Selecting the sentence and label at the specified index in the data frame\n","        sentence = self.df.loc[index, 'text']\n","        if 'label' in self.df.columns:\n","            label = self.df.loc[index, 'label']\n","        else: \n","            label = None\n","        \n","        #Preprocessing the text to be suitable for BERT\n","        tokens = self.tokenizer.tokenize(sentence) #Tokenize the sentence\n","        tokens = ['[CLS]'] + tokens + ['[SEP]'] #Insering the CLS and SEP token in the beginning and end of the sentence\n","        if len(tokens) < self.maxlen:\n","            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))] #Padding sentences\n","        else:\n","            tokens = tokens[:self.maxlen-1] + ['[SEP]'] #Prunning the list to be of specified max length\n","\n","        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens) #Obtaining the indices of the tokens in the BERT Vocabulary\n","        tokens_ids_tensor = torch.tensor(tokens_ids) #Converting the list to a pytorch tensor\n","\n","        #Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n","        attn_mask = (tokens_ids_tensor != 0).long()\n","\n","        if 'label' in self.df.columns:\n","            return tokens_ids_tensor, attn_mask, label\n","        else:\n","            return tokens_ids_tensor, attn_mask"]},{"cell_type":"code","execution_count":64,"id":"43fd184d-2118-4cbb-b248-056693beabca","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12387,"status":"ok","timestamp":1651239590250,"user":{"displayName":"Weimin Ouyang","userId":"09960439250658389642"},"user_tz":-600},"id":"43fd184d-2118-4cbb-b248-056693beabca","outputId":"4efc5022-1374-422c-be96-a2b65f5df783"},"outputs":[{"name":"stdout","output_type":"stream","text":["Done preprocessing training and development data.\n"]}],"source":["#Creating instances of training and development set\n","#maxlen sets the maximum length a sentence can have\n","#any sentence longer than this length is truncated to the maxlen size\n","train_set = SSTDataset(filename = 'train_text.csv', maxlen = 512)\n","dev_set = SSTDataset(filename = 'dev_text.csv', maxlen = 512)\n","test_set = SSTDataset(filename = 'test_text.csv', maxlen = 512)\n","\n","#Creating intsances of training and development dataloaders\n","train_loader = DataLoader(train_set, batch_size = 16, num_workers = 2)\n","dev_loader = DataLoader(dev_set, batch_size = 16, num_workers = 2)\n","test_loader = DataLoader(test_set, batch_size = 16, num_workers = 2)\n","\n","print(\"Done preprocessing training and development data.\")"]},{"cell_type":"code","execution_count":65,"id":"9b711865-2d6c-41c9-8b61-57de81e3a846","metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1651239590251,"user":{"displayName":"Weimin Ouyang","userId":"09960439250658389642"},"user_tz":-600},"id":"9b711865-2d6c-41c9-8b61-57de81e3a846"},"outputs":[],"source":["class TextClassifier(nn.Module):\n","\n","    def __init__(self):\n","        super(TextClassifier, self).__init__()\n","        #Instantiating BERT model object \n","        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n","        \n","        #Classification layer\n","        #input dimension is 768 because [CLS] embedding has a dimension of 768\n","        #output dimension is 1 because we're working with a binary classification problem\n","        self.cls_layer = nn.Linear(768, 1)\n","\n","    def forward(self, seq, attn_masks):\n","        '''\n","        Inputs:\n","            -seq : Tensor of shape [B, T] containing token ids of sequences\n","            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n","        '''\n","\n","        #Feeding the input to BERT model to obtain contextualized representations\n","        outputs = self.bert_layer(seq, attention_mask = attn_masks, return_dict=True)\n","        cont_reps = outputs.last_hidden_state\n","\n","        #Obtaining the representation of [CLS] head (the first token)\n","        cls_rep = cont_reps[:, 0]\n","\n","        #Feeding cls_rep to the classifier layer\n","        logits = self.cls_layer(cls_rep)\n","\n","        return logits"]},{"cell_type":"code","execution_count":66,"id":"e55396f8-7c48-405d-bc2e-566e23c5f427","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":498},"executionInfo":{"elapsed":5110,"status":"error","timestamp":1651239595352,"user":{"displayName":"Weimin Ouyang","userId":"09960439250658389642"},"user_tz":-600},"id":"e55396f8-7c48-405d-bc2e-566e23c5f427","outputId":"cb15a752-56ca-4cf7-caa6-bfd43719bbb1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating the text classifier, initialised with pretrained BERT-BASE parameters...\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"ename":"RuntimeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-66-a7da624ec4ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating the text classifier, initialised with pretrained BERT-BASE parameters...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done creating the sentiment classifier.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \"\"\"\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \"\"\"\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 11.17 GiB total capacity; 10.48 GiB already allocated; 33.19 MiB free; 10.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["gpu = 0 #gpu ID\n","\n","print(\"Creating the text classifier, initialised with pretrained BERT-BASE parameters...\")\n","net = TextClassifier()\n","net.cuda(gpu)\n","print(\"Done creating the sentiment classifier.\")"]},{"cell_type":"code","execution_count":null,"id":"5fb62d3b-b6be-47c0-a728-5b60b87ef61e","metadata":{"executionInfo":{"elapsed":14,"status":"aborted","timestamp":1651239595350,"user":{"displayName":"Weimin Ouyang","userId":"09960439250658389642"},"user_tz":-600},"id":"5fb62d3b-b6be-47c0-a728-5b60b87ef61e"},"outputs":[],"source":["import torch.nn as nn\n","import torch.optim as optim\n","\n","num_epoch = 7\n","\n","total_steps = total_steps = len(train_loader) * num_epoch\n","\n","# train_len = len(train)\n","# pos = sum(train.label)\n","# neg = train_len - pos\n","# pos_weight = torch.tensor([neg/pos, 1])\n","\n","criterion = nn.BCEWithLogitsLoss()\n","opti = optim.Adam(net.parameters(), lr = 5e-5, eps = 1e-8, weight_decay = 1e-2)\n","scheduler = get_linear_schedule_with_warmup(opti, num_warmup_steps = 0, num_training_steps = total_steps)"]},{"cell_type":"code","execution_count":null,"id":"c91fd811-a6c7-4dec-b9d7-ea20d8a7041d","metadata":{"executionInfo":{"elapsed":15,"status":"aborted","timestamp":1651239595351,"user":{"displayName":"Weimin Ouyang","userId":"09960439250658389642"},"user_tz":-600},"id":"c91fd811-a6c7-4dec-b9d7-ea20d8a7041d"},"outputs":[],"source":["import time\n","\n","def train(net, criterion, opti, scheduler, train_loader, dev_loader, max_eps, gpu):\n","\n","    best_acc = 0\n","    st = time.time()\n","    for ep in range(max_eps):\n","        \n","        net.train()\n","        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n","            #Clear gradients\n","            opti.zero_grad()  \n","            #Converting these to cuda tensors\n","            seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n","\n","            #Obtaining the logits from the model\n","            logits = net(seq, attn_masks)\n","\n","            #Computing loss\n","            loss = criterion(logits.squeeze(-1), labels.float())\n","\n","            #Backpropagating the gradients\n","            loss.backward()\n","\n","            #Optimization step\n","            opti.step()\n","            \n","            #Scheculer step\n","            scheduler.step()\n","              \n","            if it % 100 == 0:\n","                \n","                acc = get_accuracy_from_logits(logits, labels)\n","                print(\"Iteration {} of epoch {} complete. Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep, loss.item(), acc, (time.time()-st)))\n","                st = time.time()\n","\n","        \n","        dev_acc, dev_loss = evaluate(net, criterion, dev_loader, gpu)\n","        print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}\".format(ep, dev_acc, dev_loss))\n","        if dev_acc > best_acc:\n","            print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n","            best_acc = dev_acc\n","            torch.save(net.state_dict(), 'model_v2.dat')"]},{"cell_type":"code","execution_count":null,"id":"8602298f-4ef3-411f-aa2f-b84d5ccd306e","metadata":{"executionInfo":{"elapsed":15,"status":"aborted","timestamp":1651239595351,"user":{"displayName":"Weimin Ouyang","userId":"09960439250658389642"},"user_tz":-600},"id":"8602298f-4ef3-411f-aa2f-b84d5ccd306e"},"outputs":[],"source":["def get_accuracy_from_logits(logits, labels):\n","    probs = torch.sigmoid(logits.unsqueeze(-1))\n","    soft_probs = (probs > 0.5).long()\n","    acc = (soft_probs.squeeze() == labels).float().mean()\n","    return acc\n","\n","def evaluate(net, criterion, dataloader, gpu):\n","    net.eval()\n","\n","    mean_acc, mean_loss = 0, 0\n","    count = 0\n","\n","    with torch.no_grad():\n","        for seq, attn_masks, labels in dataloader:\n","            seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n","            logits = net(seq, attn_masks)\n","            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n","            mean_acc += get_accuracy_from_logits(logits, labels)\n","            count += 1\n","\n","    return mean_acc / count, mean_loss / count"]},{"cell_type":"code","execution_count":null,"id":"E4Okeyt9L3A0","metadata":{"executionInfo":{"elapsed":15,"status":"aborted","timestamp":1651239595351,"user":{"displayName":"Weimin Ouyang","userId":"09960439250658389642"},"user_tz":-600},"id":"E4Okeyt9L3A0"},"outputs":[],"source":["#fine-tune the model\n","train(net, criterion, opti, scheduler, train_loader, dev_loader, num_epoch, gpu)"]},{"cell_type":"code","execution_count":null,"id":"BT0nC-p1nRNv","metadata":{"executionInfo":{"elapsed":15,"status":"aborted","timestamp":1651239595351,"user":{"displayName":"Weimin Ouyang","userId":"09960439250658389642"},"user_tz":-600},"id":"BT0nC-p1nRNv"},"outputs":[],"source":["def predict(net, dataloader, gpu):\n","    net.eval()\n","\n","    final_preds = []\n","    with torch.no_grad():\n","        for seq, attn_masks in dataloader:\n","            seq, attn_masks = seq.cuda(gpu), attn_masks.cuda(gpu)\n","            logits = net(seq, attn_masks)\n","            probs = torch.sigmoid(logits.unsqueeze(-1))\n","            soft_probs = (probs > 0.5).long().squeeze().cpu().detach().numpy()\n","            final_preds.append(soft_probs)\n","    final_preds = np.hstack(final_preds)\n","\n","    return final_preds"]},{"cell_type":"code","execution_count":null,"id":"UhlXGT8U9wcJ","metadata":{"executionInfo":{"elapsed":15,"status":"aborted","timestamp":1651239595352,"user":{"displayName":"Weimin Ouyang","userId":"09960439250658389642"},"user_tz":-600},"id":"UhlXGT8U9wcJ"},"outputs":[],"source":["def get_results(net, test_loader, gpu, model_path, save_path):\n","\n","    net.load_state_dict(torch.load(model_path))\n","\n","    preds = predict(net, test_loader, gpu)\n","\n","    results = pd.DataFrame({'Predicted': preds})\n","    results.index.name = 'Id'\n","    results.to_csv(save_path)\n"]},{"cell_type":"code","execution_count":null,"id":"RRW3nMQjRGnN","metadata":{"id":"RRW3nMQjRGnN"},"outputs":[],"source":["# get_results(net, test_loader, gpu, '/content/model.dat', 'bertbase_test.csv')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"BertPlayground.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":5}
