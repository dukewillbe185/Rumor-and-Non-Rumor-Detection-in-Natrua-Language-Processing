{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cf12511-1e83-4b82-9540-31d2464c29c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision transformers emoji==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10494883-75dd-4372-bfb4-feb7a946ff6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from emoji import demojize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "176203a3-f543-49a0-81b1-dfd46b09db91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/course/chapter0/1?fw=pt\n",
    "# huggingface tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fd276be-fa37-4ee0-8e65-3fdce070599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_label.csv')\n",
    "dev = pd.read_csv('dev_label.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cc1dd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = len(train)\n",
    "dev_len = len(dev)\n",
    "test_len = len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd8df893-89a0-4d38-9d5d-95182da896c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/v4.18.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase\n",
    "# tokenizer encode_plus\n",
    "\n",
    "def tokenise(df):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-covid19-base-uncased', use_fast = False, normalization = True) # normalisation = True for raw tweets\n",
    "    \n",
    "    token_ids = []\n",
    "    attn_masks = []\n",
    "\n",
    "    for tweet in df.text:\n",
    "        batch_encoding = tokenizer.encode_plus(tweet, padding = 'max_length', truncation = True, max_length = 128, return_tensors = 'pt', return_attention_mask = True)\n",
    "        token_ids.append(batch_encoding['input_ids'])\n",
    "        attn_masks.append(batch_encoding['attention_mask'])     \n",
    "\n",
    "    token_ids = torch.cat(token_ids, 0)\n",
    "    attn_masks = torch.cat(attn_masks, 0)\n",
    "    \n",
    "    if 'label' in df.columns:\n",
    "        labels = torch.tensor(df.label)\n",
    "    else:\n",
    "        labels = None\n",
    "            \n",
    "    return token_ids, attn_masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1d20f02-e6ec-486a-a562-845ef77c0136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/what-do-tensordataset-and-dataloader-do/107017/2\n",
    "# https://blog.paperspace.com/dataloaders-abstractions-pytorch/\n",
    "# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "# for TensorDataset & DataLoaders\n",
    "\n",
    "def data_loader(train, dev, test):\n",
    "    \n",
    "    train_token_ids, train_attn_masks, train_labels = tokenise(train)\n",
    "    train_set = TensorDataset(train_token_ids, train_attn_masks, train_labels)\n",
    "    train_loader = DataLoader(train_set, batch_size = 32, num_workers = 2)\n",
    "    \n",
    "    dev_token_ids, dev_attn_masks, dev_labels = tokenise(dev)\n",
    "    dev_set = TensorDataset(dev_token_ids, dev_attn_masks, dev_labels)\n",
    "    dev_loader = DataLoader(dev_set, batch_size = 32, num_workers = 2)\n",
    "    \n",
    "    test_token_ids, test_attn_masks, _ = tokenise(test)\n",
    "    test_set = TensorDataset(test_token_ids, test_attn_masks)\n",
    "    test_loader = DataLoader(test_set, batch_size = 32, num_workers = 2)\n",
    "     \n",
    "    return train_loader, dev_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec7acb80-c280-43d1-8b48-a4775b39b88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "train_loader, dev_loader, test_loader = data_loader(train, dev, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "918453c1-0a8c-4d8d-8ff4-0594bb6db98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "num_labels = 2\n",
    "\n",
    "pos = sum(train.label)\n",
    "neg = train_len - pos\n",
    "pos_weight = torch.tensor([pos/neg, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d717b3b8-7be2-4fe4-90ea-01f1622185b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-uncased and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Wen\\Anaconda3\\envs\\weimin\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW\n",
    "# https://huggingface.co/docs/transformers/main_classes/optimizer_schedules\n",
    "  \n",
    "def model_setup(base):\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(base, num_labels = num_labels)\n",
    "\n",
    "    # lr = 1e-3, eps = 1e-8, weight_decay = 0.01\n",
    "    optimiser = AdamW(model.parameters(), lr = 1e-4)\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimiser, num_warmup_steps = 0, num_training_steps = num_training_steps)\n",
    "\n",
    "    return model, optimiser, scheduler\n",
    "\n",
    "model, optimiser, scheduler = model_setup('vinai/bertweet-covid19-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30ffdd6b-8a04-46b6-aeb6-33056055855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(logits):\n",
    "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
    "    soft_probs = probs.cpu().detach().numpy() \n",
    "    \n",
    "    pred = []\n",
    "    for item in soft_probs:\n",
    "        pred.append(np.argmax(item))\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1749aa18-edb4-458b-94b0-0ed5e15139f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(preds, labels):   \n",
    "    \n",
    "    preds = np.concatenate(preds)\n",
    "    labels = np.concatenate(labels)\n",
    "    accuracy = sum(preds == labels)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89e4becc-e5ac-4807-b169-8175631f5c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(preds, labels):\n",
    "    \n",
    "    preds = np.concatenate(preds)\n",
    "    labels = np.concatenate(labels)\n",
    "        \n",
    "    y_pred = np.array(preds)\n",
    "    y_true = np.array(labels)\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(y_true, y_pred)\n",
    "    \n",
    "    return precision[0], recall[0], fscore[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01690c3d-b3c5-4f44-9006-0df708a2a668",
   "metadata": {},
   "source": [
    "## Modified training & validation to incorporate class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f54561e8-6d31-4390-bfcf-05e0e2a0baae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dev_loader, dev_len, best_f, pos_weight, i):\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    preds = []\n",
    "    labs = []\n",
    "    val_loss = 0\n",
    "    for batch in dev_loader:\n",
    "        \n",
    "        \n",
    "        batch_token_ids, batch_attn_masks, batch_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "\n",
    "            outputs = model(input_ids = batch_token_ids, token_type_ids = None, attention_mask = batch_attn_masks)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            criterion = nn.CrossEntropyLoss(weight = pos_weight).to(device)\n",
    "            loss = criterion(logits.squeeze(-1), batch_labels)\n",
    "            \n",
    "        val_loss += loss.item()\n",
    "        \n",
    "        pred = prediction(logits)\n",
    "        preds.append(pred)\n",
    "        labs.append(batch_labels.cpu().detach().numpy())\n",
    "    \n",
    "    val_acc = get_accuracy(preds, labs)    \n",
    "    precision, recall, fscore = get_scores(preds, labs)\n",
    "    print(f'Average validation loss = {val_loss/dev_len}')\n",
    "    print(f'Average validation accuracy = {val_acc/dev_len * 100}')\n",
    "    print(f'Precision = {precision * 100}')\n",
    "    print(f'Recall = {recall * 100}')\n",
    "    print(f'F1 Socre = {fscore * 100}')\n",
    "    \n",
    "    if fscore >= best_f:\n",
    "        best_f = fscore\n",
    "        torch.save(model.state_dict(), f'bertweet_test_{i}.dat')\n",
    "        \n",
    "    return best_f\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c94ffcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('bertweet_test_10.dat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4a13c4b-6764-4520-859c-969b5e86311e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "epoch 1 / 20\n",
      "batch 10 / 50 Time: 0:00:06.357470\n",
      "batch 20 / 50 Time: 0:00:11.080929\n",
      "batch 30 / 50 Time: 0:00:15.806401\n",
      "batch 40 / 50 Time: 0:00:20.561426\n",
      "Average training loss = 0.015358556835514724\n",
      "Average training accuracy = 74.71337579617834\n",
      "Epoch time = 0:00:24.615217\n",
      "Average validation loss = 0.009205790746478395\n",
      "Average validation accuracy = 92.17877094972067\n",
      "Precision = 95.23809523809523\n",
      "Recall = 94.7867298578199\n",
      "F1 Socre = 95.01187648456056\n",
      "=========================\n",
      "epoch 2 / 20\n",
      "batch 10 / 50 Time: 0:00:06.295013\n",
      "batch 20 / 50 Time: 0:00:11.100699\n",
      "batch 30 / 50 Time: 0:00:15.877753\n",
      "batch 40 / 50 Time: 0:00:20.667286\n",
      "Average training loss = 0.0055734406545710794\n",
      "Average training accuracy = 94.71337579617834\n",
      "Epoch time = 0:00:24.747463\n",
      "Average validation loss = 0.005700916343228111\n",
      "Average validation accuracy = 96.8342644320298\n",
      "Precision = 97.42388758782201\n",
      "Recall = 98.5781990521327\n",
      "F1 Socre = 97.99764428739694\n",
      "=========================\n",
      "epoch 3 / 20\n",
      "batch 10 / 50 Time: 0:00:06.349091\n",
      "batch 20 / 50 Time: 0:00:11.132605\n",
      "batch 30 / 50 Time: 0:00:15.915244\n",
      "batch 40 / 50 Time: 0:00:20.728884\n",
      "Average training loss = 0.0032567897546075426\n",
      "Average training accuracy = 97.51592356687898\n",
      "Epoch time = 0:00:24.827816\n",
      "Average validation loss = 0.005139931861668464\n",
      "Average validation accuracy = 95.7169459962756\n",
      "Precision = 98.30508474576271\n",
      "Recall = 96.2085308056872\n",
      "F1 Socre = 97.24550898203593\n",
      "=========================\n",
      "epoch 4 / 20\n",
      "batch 10 / 50 Time: 0:00:06.340110\n",
      "batch 20 / 50 Time: 0:00:11.153165\n",
      "batch 30 / 50 Time: 0:00:15.973082\n",
      "batch 40 / 50 Time: 0:00:20.797501\n",
      "Average training loss = 0.0021649904445312017\n",
      "Average training accuracy = 98.9171974522293\n",
      "Epoch time = 0:00:24.919507\n",
      "Average validation loss = 0.007653967155667379\n",
      "Average validation accuracy = 96.8342644320298\n",
      "Precision = 97.6470588235294\n",
      "Recall = 98.34123222748815\n",
      "F1 Socre = 97.99291617473435\n",
      "=========================\n",
      "epoch 5 / 20\n",
      "batch 10 / 50 Time: 0:00:06.188604\n",
      "batch 20 / 50 Time: 0:00:11.005247\n",
      "batch 30 / 50 Time: 0:00:15.871286\n",
      "batch 40 / 50 Time: 0:00:20.710753\n",
      "Average training loss = 0.0005941975448031429\n",
      "Average training accuracy = 99.61783439490446\n",
      "Epoch time = 0:00:24.838992\n",
      "Average validation loss = 0.006429222021538078\n",
      "Average validation accuracy = 97.20670391061452\n",
      "Precision = 98.56801909307876\n",
      "Recall = 97.86729857819904\n",
      "F1 Socre = 98.21640903686088\n",
      "=========================\n",
      "epoch 6 / 20\n",
      "batch 10 / 50 Time: 0:00:06.393832\n",
      "batch 20 / 50 Time: 0:00:11.373134\n",
      "batch 30 / 50 Time: 0:00:16.391056\n",
      "batch 40 / 50 Time: 0:00:21.290707\n",
      "Average training loss = 0.00012168635845563974\n",
      "Average training accuracy = 99.87261146496816\n",
      "Epoch time = 0:00:25.498081\n",
      "Average validation loss = 0.005835912688828231\n",
      "Average validation accuracy = 97.20670391061452\n",
      "Precision = 98.33729216152018\n",
      "Recall = 98.10426540284361\n",
      "F1 Socre = 98.22064056939502\n",
      "=========================\n",
      "epoch 7 / 20\n",
      "batch 10 / 50 Time: 0:00:06.556819\n",
      "batch 20 / 50 Time: 0:00:11.583401\n",
      "batch 30 / 50 Time: 0:00:16.616780\n",
      "batch 40 / 50 Time: 0:00:21.636663\n",
      "Average training loss = 1.2018046917239571e-05\n",
      "Average training accuracy = 100.0\n",
      "Epoch time = 0:00:25.801399\n",
      "Average validation loss = 0.008600436660696993\n",
      "Average validation accuracy = 97.39292364990689\n",
      "Precision = 98.34123222748815\n",
      "Recall = 98.34123222748815\n",
      "F1 Socre = 98.34123222748815\n",
      "=========================\n",
      "epoch 8 / 20\n",
      "batch 10 / 50 Time: 0:00:06.405054\n",
      "batch 20 / 50 Time: 0:00:11.240510\n",
      "batch 30 / 50 Time: 0:00:16.098389\n",
      "batch 40 / 50 Time: 0:00:20.961475\n",
      "Average training loss = 8.714843688200518e-06\n",
      "Average training accuracy = 100.0\n",
      "Epoch time = 0:00:25.120239\n",
      "Average validation loss = 0.0072244213864147\n",
      "Average validation accuracy = 97.76536312849163\n",
      "Precision = 98.5781990521327\n",
      "Recall = 98.5781990521327\n",
      "F1 Socre = 98.5781990521327\n",
      "=========================\n",
      "epoch 9 / 20\n",
      "batch 10 / 50 Time: 0:00:06.370640\n",
      "batch 20 / 50 Time: 0:00:11.243215\n",
      "batch 30 / 50 Time: 0:00:16.092106\n",
      "batch 40 / 50 Time: 0:00:20.978391\n",
      "Average training loss = 6.816346280997512e-06\n",
      "Average training accuracy = 100.0\n",
      "Epoch time = 0:00:25.137323\n",
      "Average validation loss = 0.008096933276554027\n",
      "Average validation accuracy = 97.57914338919925\n",
      "Precision = 98.57482185273159\n",
      "Recall = 98.34123222748815\n",
      "F1 Socre = 98.45788849347569\n",
      "=========================\n",
      "epoch 10 / 20\n",
      "batch 10 / 50 Time: 0:00:06.402984\n",
      "batch 20 / 50 Time: 0:00:11.334977\n",
      "batch 30 / 50 Time: 0:00:16.291858\n",
      "batch 40 / 50 Time: 0:00:21.165352\n",
      "Average training loss = 5.624786745623395e-06\n",
      "Average training accuracy = 100.0\n",
      "Epoch time = 0:00:25.324214\n",
      "Average validation loss = 0.009180667323127105\n",
      "Average validation accuracy = 97.57914338919925\n",
      "Precision = 98.3451536643026\n",
      "Recall = 98.5781990521327\n",
      "F1 Socre = 98.46153846153845\n",
      "=========================\n",
      "epoch 11 / 20\n",
      "batch 10 / 50 Time: 0:00:06.361815\n",
      "batch 20 / 50 Time: 0:00:11.225582\n",
      "batch 30 / 50 Time: 0:00:16.107759\n",
      "batch 40 / 50 Time: 0:00:21.011008\n",
      "Average training loss = 4.77518045097281e-06\n",
      "Average training accuracy = 100.0\n",
      "Epoch time = 0:00:25.161682\n",
      "Average validation loss = 0.008920616466808697\n",
      "Average validation accuracy = 97.57914338919925\n",
      "Precision = 98.3451536643026\n",
      "Recall = 98.5781990521327\n",
      "F1 Socre = 98.46153846153845\n",
      "=========================\n",
      "epoch 12 / 20\n",
      "batch 10 / 50 Time: 0:00:06.393598\n",
      "batch 20 / 50 Time: 0:00:11.281221\n",
      "batch 30 / 50 Time: 0:00:16.166553\n",
      "batch 40 / 50 Time: 0:00:21.042611\n",
      "Average training loss = 4.237015840262293e-06\n",
      "Average training accuracy = 100.0\n",
      "Epoch time = 0:00:25.218557\n",
      "Average validation loss = 0.008530785362158158\n",
      "Average validation accuracy = 97.57914338919925\n",
      "Precision = 98.57482185273159\n",
      "Recall = 98.34123222748815\n",
      "F1 Socre = 98.45788849347569\n",
      "=========================\n",
      "epoch 13 / 20\n",
      "batch 10 / 50 Time: 0:00:06.411275\n",
      "batch 20 / 50 Time: 0:00:11.302704\n",
      "batch 30 / 50 Time: 0:00:16.185645\n",
      "batch 40 / 50 Time: 0:00:21.096735\n",
      "Average training loss = 3.816936411457374e-06\n",
      "Average training accuracy = 100.0\n",
      "Epoch time = 0:00:25.361518\n",
      "Average validation loss = 0.010956807243210668\n",
      "Average validation accuracy = 97.20670391061452\n",
      "Precision = 97.88235294117648\n",
      "Recall = 98.5781990521327\n",
      "F1 Socre = 98.22904368358914\n",
      "=========================\n",
      "epoch 14 / 20\n",
      "batch 10 / 50 Time: 0:00:06.494636\n",
      "batch 20 / 50 Time: 0:00:11.219031\n",
      "batch 30 / 50 Time: 0:00:16.079937\n",
      "batch 40 / 50 Time: 0:00:20.960421\n",
      "Average training loss = 3.4477885932643796e-06\n",
      "Average training accuracy = 100.0\n",
      "Epoch time = 0:00:25.145928\n",
      "Average validation loss = 0.007970454258638524\n",
      "Average validation accuracy = 97.57914338919925\n",
      "Precision = 98.80668257756562\n",
      "Recall = 98.10426540284361\n",
      "F1 Socre = 98.45422116527944\n",
      "=========================\n",
      "epoch 15 / 20\n",
      "batch 10 / 50 Time: 0:00:06.418035\n",
      "batch 20 / 50 Time: 0:00:11.328957\n",
      "batch 30 / 50 Time: 0:00:16.202638\n",
      "batch 40 / 50 Time: 0:00:21.074423\n",
      "Average training loss = 3.215260879703127e-06\n",
      "Average training accuracy = 100.0\n",
      "Epoch time = 0:00:25.233489\n",
      "Average validation loss = 0.00879685949385732\n",
      "Average validation accuracy = 97.20670391061452\n",
      "Precision = 98.10874704491725\n",
      "Recall = 98.34123222748815\n",
      "F1 Socre = 98.22485207100591\n",
      "=========================\n",
      "epoch 16 / 20\n",
      "batch 10 / 50 Time: 0:00:07.382675\n",
      "batch 20 / 50 Time: 0:00:12.302996\n",
      "batch 30 / 50 Time: 0:00:17.215321\n",
      "batch 40 / 50 Time: 0:00:22.129972\n",
      "Average training loss = 2.989654432408286e-06\n",
      "Average training accuracy = 100.0\n",
      "Epoch time = 0:00:26.284871\n",
      "Average validation loss = 0.010747220976205778\n",
      "Average validation accuracy = 97.57914338919925\n",
      "Precision = 98.3451536643026\n",
      "Recall = 98.5781990521327\n",
      "F1 Socre = 98.46153846153845\n",
      "=========================\n",
      "epoch 17 / 20\n",
      "batch 10 / 50 Time: 0:00:06.398989\n",
      "batch 20 / 50 Time: 0:00:11.267488\n",
      "batch 30 / 50 Time: 0:00:16.146302\n",
      "batch 40 / 50 Time: 0:00:21.062778\n",
      "Average training loss = 2.8616367249043695e-06\n",
      "Average training accuracy = 100.0\n",
      "Epoch time = 0:00:25.261747\n",
      "Average validation loss = 0.010613213527968367\n",
      "Average validation accuracy = 97.20670391061452\n",
      "Precision = 97.88235294117648\n",
      "Recall = 98.5781990521327\n",
      "F1 Socre = 98.22904368358914\n",
      "=========================\n",
      "epoch 18 / 20\n",
      "batch 10 / 50 Time: 0:00:06.404097\n",
      "batch 20 / 50 Time: 0:00:11.312389\n",
      "batch 30 / 50 Time: 0:00:16.236069\n",
      "batch 40 / 50 Time: 0:00:21.175225\n",
      "Average training loss = 2.7874010933153807e-06\n",
      "Average training accuracy = 100.0\n",
      "Epoch time = 0:00:25.388303\n",
      "Average validation loss = 0.008839305893874539\n",
      "Average validation accuracy = 97.57914338919925\n",
      "Precision = 98.57482185273159\n",
      "Recall = 98.34123222748815\n",
      "F1 Socre = 98.45788849347569\n",
      "=========================\n",
      "epoch 19 / 20\n",
      "batch 10 / 50 Time: 0:00:06.317176\n",
      "batch 20 / 50 Time: 0:00:11.175264\n",
      "batch 30 / 50 Time: 0:00:16.026227\n",
      "batch 40 / 50 Time: 0:00:20.904776\n",
      "Average training loss = 2.7175948098316717e-06\n",
      "Average training accuracy = 100.0\n",
      "Epoch time = 0:00:25.043982\n",
      "Average validation loss = 0.00919970728986371\n",
      "Average validation accuracy = 97.57914338919925\n",
      "Precision = 98.3451536643026\n",
      "Recall = 98.5781990521327\n",
      "F1 Socre = 98.46153846153845\n",
      "=========================\n",
      "epoch 20 / 20\n",
      "batch 10 / 50 Time: 0:00:06.376937\n",
      "batch 20 / 50 Time: 0:00:11.245520\n",
      "batch 30 / 50 Time: 0:00:16.117394\n",
      "batch 40 / 50 Time: 0:00:20.976093\n",
      "Average training loss = 2.657692017631344e-06\n",
      "Average training accuracy = 100.0\n",
      "Epoch time = 0:00:25.136724\n",
      "Average validation loss = 0.008554978957947658\n",
      "Average validation accuracy = 97.57914338919925\n",
      "Precision = 98.57482185273159\n",
      "Recall = 98.34123222748815\n",
      "F1 Socre = 98.45788849347569\n",
      "Total time = 0:09:54.797005\n"
     ]
    }
   ],
   "source": [
    "# https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "# https://discuss.huggingface.co/t/class-weights-for-bertforsequenceclassification/1674/5 (class weights)\n",
    "\n",
    "def train(model, optimiser, scheduler, train_loader, dev_loader, num_epochs, train_len, dev_len, pos_weight):\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    best_f = 0\n",
    "    total_start = datetime.now()\n",
    "    for i in range(num_epochs):\n",
    "        print('=' * 25)\n",
    "        print(f'epoch {i + 1} / {num_epochs}')\n",
    "\n",
    "        \n",
    "        training_loss = 0\n",
    "        training_acc = 0\n",
    "        epoch_start = datetime.now()\n",
    "        model.train()\n",
    "        for num, batch in enumerate(train_loader):\n",
    "            batch_preds = []\n",
    "            batch_labs = []\n",
    "            \n",
    "            optimiser.zero_grad()\n",
    "                \n",
    "            batch_token_ids, batch_attn_masks, batch_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
    "\n",
    "            outputs = model(input_ids = batch_token_ids, token_type_ids = None, attention_mask = batch_attn_masks)\n",
    " \n",
    "            logits = outputs.logits\n",
    "            pred = prediction(logits)\n",
    "            batch_preds.append(pred)\n",
    "            \n",
    "            criterion = nn.CrossEntropyLoss(weight = pos_weight).to(device)\n",
    "            loss = criterion(logits.squeeze(-1), batch_labels)\n",
    "            training_loss += loss.item()\n",
    "\n",
    "            batch_labs.append(batch_labels.cpu().detach().numpy())\n",
    "            \n",
    "            batch_acc = get_accuracy(batch_preds, batch_labs)\n",
    "            training_acc += batch_acc\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimiser.step()\n",
    "            scheduler.step()\n",
    "                      \n",
    "            if num > 0 and num % 10 == 0:\n",
    "                time = datetime.now() - epoch_start\n",
    "                print(f'batch {num} / {len(train_loader)} Time: {time}')\n",
    "        \n",
    "        \n",
    "        epoch_time = datetime.now() - epoch_start\n",
    "        print(f'Average training loss = {training_loss/train_len}')\n",
    "        print(f'Average training accuracy = {training_acc/train_len * 100}')\n",
    "        print(f'Epoch time = {epoch_time}')\n",
    "        \n",
    "        best_fscore = validate(model, dev_loader, dev_len, best_f, pos_weight, i)\n",
    "        best_f = best_fscore\n",
    "    \n",
    "    total_time = datetime.now() - total_start\n",
    "    print(f'Total time = {total_time}')\n",
    "\n",
    "train(model, optimiser, scheduler, train_loader, dev_loader, num_epochs, train_len, dev_len, pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ab9f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "\n",
    "    preds = []\n",
    "    for batch in test_loader:\n",
    "        batch_token_ids, batch_attn_masks = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=batch_token_ids, token_type_ids=None, attention_mask=batch_attn_masks)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            pred = prediction(logits)\n",
    "            preds.append(pred)\n",
    "\n",
    "    preds = np.hstack(preds)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c52afa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(model, test_loader, model_path, save_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    preds = predict(model, test_loader)\n",
    "\n",
    "    results = pd.DataFrame({'Predicted': preds})\n",
    "    results.index.name = 'Id'\n",
    "    results.to_csv(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bbd9b937",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_results(model, test_loader, 'bertweet_test_7.dat', 'bertweet_test_v4.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
