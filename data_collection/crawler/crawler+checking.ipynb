{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tweepy\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# BEARER_TOKEN = 'AAAAAAAAAAAAAAAAAAAAAGvGbQEAAAAACMRCC9dW75oHLdZY872KQV%2FwncY%3DAYzORvis40n3ndPKneBisKPBQ4sYVpNthL0YHJLJZ9d6WmAjWY'\n",
    "# BEARER_TOKEN = 'AAAAAAAAAAAAAAAAAAAAAAoxbAEAAAAApib47uKVACiyXfOSAj8BELeJaf4%3DT4L0eLVTNooQRZVftkppGGnEoRqau3K2R73ZJYo3jB8TZFghZJ'\n",
    "# BEARER_TOKEN = 'AAAAAAAAAAAAAAAAAAAAAGjcbAEAAAAAnAt%2FNMSgejEX0q4HaxMGYfwExWU%3DT4mLNCEmm8oJxrCYbGGYDzEvuFwHPr4VHYvrzXJooQLTyeAbe2'\n",
    "BEARER_TOKEN = 'AAAAAAAAAAAAAAAAAAAAACvAbgEAAAAAZTSFcdm27S5RfVV%2BCjCzRgcI1Ko%3DVJV80daHMDeiwLxxb9kxCWUZthIjLLFQ3FwCFXdBaNTcdILng7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate(BEARER_TOKEN):\n",
    "\n",
    "    client = tweepy.Client(bearer_token=BEARER_TOKEN, wait_on_rate_limit=True)\n",
    "\n",
    "    return client\n",
    "\n",
    "client = authenticate(BEARER_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_tweet(client, id):\n",
    "\n",
    "    tweet = {}\n",
    "    response = client.get_tweets(id, \n",
    "                                tweet_fields = ['attachments', 'created_at', 'entities', 'geo', \n",
    "                                                'public_metrics', 'referenced_tweets', \n",
    "                                                'author_id', 'source', 'context_annotations', 'in_reply_to_user_id', 'lang'],\n",
    "                                place_fields = ['geo', 'country', 'name', 'place_type'],\n",
    "                                user_fields = ['created_at', 'description','entities', 'public_metrics', 'verified'],\n",
    "                                media_fields = ['url', 'duration_ms', 'public_metrics', 'alt_text'],\n",
    "                                expansions = 'geo.place_id,author_id,attachments.media_keys,in_reply_to_user_id')\n",
    "    try:\n",
    "        if response.data is not None:\n",
    "            for t in response.data:\n",
    "                tweet.update(t.data)\n",
    "    except Exception as e:\n",
    "        print(f'Exception for tweet: {e}')\n",
    "        pass\n",
    "    try:\n",
    "        if response.includes.get('users', None) is not None:   \n",
    "            for user in response.includes['users']:\n",
    "                tweet['user'] = user.data\n",
    "    except Exception as e:\n",
    "        print(f'Exception for users: {e}')\n",
    "        pass\n",
    "    try:\n",
    "        if response.includes.get('place', None) is not None:\n",
    "            for place in response.includes['place']:\n",
    "                tweet.update(place.data)\n",
    "    except Exception as e:\n",
    "        print(f'Exception for place: {e}')\n",
    "        pass\n",
    "    try:\n",
    "        if response.includes.get('media', None) is not None:\n",
    "            for media in response.includes['media']:\n",
    "                tweet.update(media.data)\n",
    "    except Exception as e:\n",
    "        print('f Exception for media: {e}')\n",
    "        pass\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(id, tweet, path):\n",
    "    with open(path + id +'.txt', 'w') as f:\n",
    "        json.dump(tweet, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets_processed(filepath):\n",
    "\n",
    "    ids = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.split(' : ')\n",
    "            ids.append(line[0])\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_process(client, r_path, w_path, logpath):\n",
    "\n",
    "    ids = tweets_processed(logpath)   \n",
    "     \n",
    "    with open(r_path, 'r', encoding = 'utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip('\\n')\n",
    "            line = line.split(',')\n",
    "            for id in line:\n",
    "                if id not in ids:\n",
    "                    try:\n",
    "                    \n",
    "                        tweet = fetch_tweet(client, id)   \n",
    "\n",
    "                        write_to_file(id, tweet, w_path)\n",
    "\n",
    "                        log = id + ' : yes\\n'\n",
    "                        with open(logpath, 'a') as f:\n",
    "                            f.write(log)  \n",
    "                    except Exception as e:\n",
    "                        print(f'Exception for fetch and write: {e}')\n",
    "                        print(id)\n",
    "                        pass\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_process(client, 'train.data.txt', 'tweet-objects-train/', 'log.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saved (log_path):\n",
    "\n",
    "    saved = []\n",
    "    with open(log_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.split(' : ')\n",
    "            saved.append(line[0])\n",
    "    return saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all (data_path):\n",
    "    ids = []\n",
    "    with open(data_path, 'r', encoding = 'utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip('\\n')\n",
    "            line = line.split(',')\n",
    "            for id in line:\n",
    "                ids.append(id)\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'train.data.txt'\n",
    "log_path = 'log.txt'\n",
    "\n",
    "log = saved(log_path)\n",
    "total = all(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(np.unique(log)) != len(np.unique(total)):\n",
    "    for id in total:\n",
    "        if id not in log:\n",
    "            try:\n",
    "                tweet = fetch_tweet(client, id)   \n",
    "                write_to_file(id, tweet, 'tweet-objects-train/')\n",
    "                log = id + ' : yes\\n'\n",
    "                with open('log.txt', 'a') as f:\n",
    "                    f.write(log)  \n",
    "            except Exception as e:\n",
    "                print(f'Exception for fetch and write: {e}')\n",
    "                print(id)\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = []\n",
    "\n",
    "path = '/Users/weimin/Downloads/NLP/project-data/data_obj/train/tweet-objects-train'\n",
    "\n",
    "os.chdir(path)\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    with open (file_path, 'r') as f:\n",
    "        if f.read() == '{}':\n",
    "            f_name = file_path.split('/')\n",
    "            name = f_name[-1].split('.')\n",
    "            check.append(name[0])\n",
    "\n",
    "for file in os.listdir():\n",
    "    if file.endswith('.txt'):\n",
    "        file_path = f'{path}/{file}'\n",
    "        read_text_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8262\n"
     ]
    }
   ],
   "source": [
    "print(len(check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8305\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/weimin/Downloads/NLP/project-data/data_collection/crawler'\n",
    "\n",
    "os.chdir(path)\n",
    "\n",
    "checked = []\n",
    "with open('check-log.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.split(' : ')\n",
    "        checked.append(line[0])\n",
    "\n",
    "print(len(checked))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 400 seconds.\n"
     ]
    }
   ],
   "source": [
    "for id in check:\n",
    "    if id not in checked:\n",
    "        try: \n",
    "            tweet = fetch_tweet(client, id)   \n",
    "            if tweet != {}:\n",
    "                print(tweet)\n",
    "                print(id)\n",
    "\n",
    "                write_to_file(id, tweet, 'tweet-objects-train/')\n",
    "\n",
    "            elif tweet == {}:\n",
    "                log = id + ' : yes\\n'\n",
    "                with open('check-log.txt', 'a') as f:\n",
    "                    f.write(log)  \n",
    "        except Exception as e:\n",
    "            print(f'Exception for fetch and write: {e}')\n",
    "            print(id)\n",
    "            pass"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0dabc2a3d58a0c5bc25b4ec180f9b0e69ae1522730256f35a8f23b249fc9a485"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
