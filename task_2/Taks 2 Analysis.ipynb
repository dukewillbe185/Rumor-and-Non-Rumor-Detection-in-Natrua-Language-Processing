{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b21a0cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dukeisyourdaddy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import nltk; nltk.download('stopwords')\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "64a5c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5de116d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_label = pd.read_csv('/Users/dukeisyourdaddy/Desktop/covid_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "fa3aac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#covid_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "faaf3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#covid_unru = covid_label[x if x==0 in ['Predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "db538c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#covid_label.iloc[1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a6057f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#covid_label.iloc[0]['Predicted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "32a57078",
   "metadata": {},
   "outputs": [],
   "source": [
    "#covid_label.iloc[2]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "08327d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(covid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6ba5d53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_ru = []\n",
    "covid_unru = []\n",
    "for i in range(len(covid_label)):\n",
    "    if covid_label.iloc[i]['Predicted'] == 0:\n",
    "        covid_unru.append(covid_label.iloc[i]['text'])\n",
    "    else:\n",
    "        covid_ru.append(covid_label.iloc[i]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "dc9223ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted(list_a):\n",
    "    tt = TweetTokenizer()\n",
    "    #covid_ru_1 = tt.tokenize(covid_ru[1])\n",
    "    hashtag = {}\n",
    "\n",
    "    for element in list_a:\n",
    "        token_element = tt.tokenize(element)\n",
    "        for item in token_element:\n",
    "            if '#' in item:\n",
    "                if item not in hashtag:\n",
    "                    hashtag[item] = 1\n",
    "                else:\n",
    "                    hashtag[item] += 1\n",
    "                    \n",
    "    hash_value = sum(hashtag.values())\n",
    "\n",
    "    # Create a list of tuples sorted by index 1 i.e. value field     \n",
    "    listofTuples = sorted(hashtag.items() , reverse=True, key=lambda x: x[1])\n",
    "    # Iterate over the sorted sequence\n",
    "    sorted_result = []\n",
    "    for elem in listofTuples :\n",
    "        sorted_result.append((str(elem[0]) +\" :\" + str(elem[1]) ))\n",
    "    \n",
    "    return sorted_result,hash_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4e2d493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_rumor, rumor_value = get_sorted(covid_ru)\n",
    "hash_unrumor, unru_value = get_sorted(covid_unru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "71c64e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#COVID19 :4186',\n",
       " '#coronavirus :2398',\n",
       " '#Trump :472',\n",
       " '#Coronavirus :365',\n",
       " '#Covid19 :362',\n",
       " '#covid19 :300',\n",
       " '#WuhanVirus :276',\n",
       " '# :264',\n",
       " '#TrumpVirus :222',\n",
       " '#CoronavirusPandemic :207',\n",
       " '#CoronaVirus :168']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_rumor[0:11] #select 0:11 cuz we won't count #(lazy to remove it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "50e6e2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#COVID19 :12441',\n",
       " '#coronavirus :6056',\n",
       " '#covid19 :1665',\n",
       " '#Covid19 :1466',\n",
       " '#Coronavirus :1192',\n",
       " '#Covid_19 :560',\n",
       " '#CoronavirusPandemic :558',\n",
       " '#WuhanVirus :539',\n",
       " '# :539',\n",
       " '#Trump :532',\n",
       " '#COVIDãƒ¼19 :522']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_unrumor[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "332af882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "            \n",
    "    tweet_clean = ' '.join(tweets_clean)\n",
    "\n",
    "    return tweet_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "449cc775",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_covid_ru = [process_tweet(t) for t in covid_ru]\n",
    "process_covid_unru = [process_tweet(t) for t in covid_unru]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2f4774d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA(corpus):\n",
    "    # Convert to list\n",
    "    data = corpus\n",
    "\n",
    "    # Remove Emails\n",
    "    data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "    def sent_to_words(sentences):\n",
    "        for sentence in sentences:\n",
    "            yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "    data_words = list(sent_to_words(data))\n",
    "\n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    # Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "    def remove_stopwords(texts):\n",
    "        return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "    def make_bigrams(texts):\n",
    "        return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "    def make_trigrams(texts):\n",
    "        return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "    def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "        \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "        texts_out = []\n",
    "        for sent in texts:\n",
    "            doc = nlp(\" \".join(sent)) \n",
    "            texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "        return texts_out\n",
    "\n",
    "    # Remove Stop Words\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "    # python3 -m spacy download en\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts = data_lemmatized\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    # Build LDA model\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=10, \n",
    "                                               random_state=100,\n",
    "                                               update_every=1,\n",
    "                                               chunksize=100,\n",
    "                                               passes=10,\n",
    "                                               alpha='auto',\n",
    "                                               per_word_topics=True)\n",
    "\n",
    "\n",
    "    # Print the Keyword in the 10 topics\n",
    "    pprint(lda_model.print_topics())\n",
    "    doc_lda = lda_model[corpus]\n",
    "    \n",
    "    # Compute Perplexity\n",
    "    print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('\\nCoherence Score: ', coherence_lda)\n",
    "    \n",
    "    return doc_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e3664c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.105*\"trump\" + 0.049*\"respon\" + 0.025*\"coronaviru\" + 0.021*\"presid\" + '\n",
      "  '0.017*\"crisi\" + 0.017*\"obama\" + 0.017*\"brief\" + 0.015*\"instead\" + '\n",
      "  '0.013*\"american\" + 0.012*\"fight\"'),\n",
      " (1,\n",
      "  '0.041*\"covid\" + 0.037*\"coronaviru\" + 0.026*\"get\" + 0.021*\"die\" + 0.018*\"go\" '\n",
      "  '+ 0.013*\"take\" + 0.012*\"peopl\" + 0.010*\"make\" + 0.009*\"kill\" + '\n",
      "  '0.009*\"think\"'),\n",
      " (2,\n",
      "  '0.022*\"rank\" + 0.020*\"tower\" + 0.019*\"jake\" + 0.019*\"russian_collus\" + '\n",
      "  '0.006*\"paywal\" + 0.006*\"clip\" + 0.003*\"ilk\" + 0.003*\"rupert_murdoch\" + '\n",
      "  '0.003*\"hey\" + 0.002*\"cue\"'),\n",
      " (3,\n",
      "  '0.035*\"coronaviru\" + 0.032*\"death\" + 0.023*\"covid\" + 0.020*\"say\" + '\n",
      "  '0.019*\"test\" + 0.015*\"case\" + 0.015*\"trump\" + 0.011*\"number\" + 0.010*\"know\" '\n",
      "  '+ 0.008*\"flu\"'),\n",
      " (4,\n",
      "  '0.030*\"prison\" + 0.026*\"relea\" + 0.021*\"asthma\" + 0.010*\"inmat\" + '\n",
      "  '0.007*\"chairman\" + 0.005*\"confin\" + 0.005*\"sentenc\" + '\n",
      "  '0.005*\"conspiraci_theorist\" + 0.004*\"tiktok\" + 0.003*\"prosecutor\"'),\n",
      " (5,\n",
      "  '0.019*\"awak\" + 0.014*\"congression\" + 0.011*\"ron_desanti\" + 0.009*\"investor\" '\n",
      "  '+ 0.006*\"votethemout\" + 0.004*\"politico\" + 0.003*\"jare_kushner\" + '\n",
      "  '0.003*\"repercuss\" + 0.003*\"grandfath\" + 0.002*\"tori\"'),\n",
      " (6,\n",
      "  '0.064*\"imagin\" + 0.023*\"washington_post\" + 0.021*\"backlash\" + '\n",
      "  '0.017*\"desanti\" + 0.012*\"progress\" + 0.012*\"alarm\" + 0.009*\"focus\" + '\n",
      "  '0.007*\"peddl\" + 0.007*\"single\" + 0.006*\"transpar\"'),\n",
      " (7,\n",
      "  '0.027*\"covid\" + 0.026*\"job\" + 0.017*\"dead\" + 0.016*\"back\" + 0.016*\"go\" + '\n",
      "  '0.015*\"state\" + 0.014*\"help\" + 0.014*\"vote\" + 0.014*\"american\" + '\n",
      "  '0.014*\"economi\"'),\n",
      " (8,\n",
      "  '0.018*\"helm\" + 0.016*\"lamestream\" + 0.010*\"memo\" + 0.010*\"wuhan\" + '\n",
      "  '0.007*\"rightli\" + 0.005*\"transcript\" + 0.004*\"grifter\" + 0.004*\"elector\" + '\n",
      "  '0.004*\"thwart\" + 0.004*\"vein\"'),\n",
      " (9,\n",
      "  '0.049*\"protest\" + 0.041*\"covid\" + 0.022*\"spread\" + 0.022*\"ralli\" + '\n",
      "  '0.018*\"child\" + 0.016*\"attend\" + 0.015*\"get\" + 0.013*\"go\" + 0.013*\"school\" '\n",
      "  '+ 0.012*\"crowd\"')]\n",
      "\n",
      "Perplexity:  -7.438994440330689\n",
      "\n",
      "Coherence Score:  0.45535412647807244\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x7f9b304e6220>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA(process_covid_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d7316e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.073*\"child\" + 0.072*\"school\" + 0.041*\"student\" + 0.033*\"open\" + '\n",
      "  '0.033*\"kid\" + 0.023*\"univer\" + 0.017*\"teacher\" + 0.017*\"resign\" + '\n",
      "  '0.015*\"parent\" + 0.014*\"reopen\"'),\n",
      " (1,\n",
      "  '0.033*\"extend\" + 0.022*\"muslim\" + 0.020*\"nigerian\" + 0.019*\"coronavirusupd\" '\n",
      "  '+ 0.019*\"lift\" + 0.016*\"speech\" + 0.014*\"disappoint\" + 0.014*\"madagascar\" + '\n",
      "  '0.014*\"fight\" + 0.014*\"sir\"'),\n",
      " (2,\n",
      "  '0.073*\"covid\" + 0.028*\"get\" + 0.022*\"go\" + 0.017*\"die\" + 0.016*\"take\" + '\n",
      "  '0.014*\"peopl\" + 0.011*\"see\" + 0.011*\"coronaviru\" + 0.011*\"know\" + '\n",
      "  '0.010*\"care\"'),\n",
      " (3,\n",
      "  '0.046*\"help\" + 0.044*\"tesla\" + 0.041*\"bill\" + 0.040*\"employe\" + 0.032*\"due\" '\n",
      "  '+ 0.030*\"park\" + 0.020*\"job\" + 0.020*\"money\" + 0.019*\"food\" + '\n",
      "  '0.014*\"payment\"'),\n",
      " (4,\n",
      "  '0.060*\"restaur\" + 0.040*\"migrant\" + 0.018*\"plagu\" + 0.017*\"queen\" + '\n",
      "  '0.011*\"injustic\" + 0.010*\"hundr\" + 0.010*\"bloodi\" + 0.008*\"recruit\" + '\n",
      "  '0.007*\"tobacco\" + 0.007*\"left_we\"'),\n",
      " (5,\n",
      "  '0.050*\"coronaviru\" + 0.021*\"say\" + 0.014*\"trump\" + 0.010*\"make\" + '\n",
      "  '0.010*\"respon\" + 0.010*\"countri\" + 0.009*\"stop\" + 0.009*\"viru\" + '\n",
      "  '0.008*\"need\" + 0.008*\"world\"'),\n",
      " (6,\n",
      "  '0.094*\"vaccin\" + 0.073*\"treatment\" + 0.047*\"develop\" + 0.046*\"drug\" + '\n",
      "  '0.020*\"market\" + 0.017*\"parliament\" + 0.017*\"treat\" + 0.017*\"anim\" + '\n",
      "  '0.017*\"cure\" + 0.013*\"trial\"'),\n",
      " (7,\n",
      "  '0.074*\"death\" + 0.073*\"test\" + 0.067*\"case\" + 0.037*\"covid\" + 0.029*\"new\" + '\n",
      "  '0.027*\"number\" + 0.025*\"state\" + 0.023*\"day\" + 0.020*\"posit\" + '\n",
      "  '0.016*\"report\"'),\n",
      " (8,\n",
      "  '0.067*\"govern\" + 0.048*\"lockdown\" + 0.031*\"travel\" + 0.023*\"rule\" + '\n",
      "  '0.021*\"drive\" + 0.018*\"break\" + 0.017*\"home\" + 0.013*\"govt\" + 0.013*\"car\" + '\n",
      "  '0.011*\"wife\"'),\n",
      " (9,\n",
      "  '0.106*\"critic\" + 0.021*\"ice\" + 0.020*\"jump\" + 0.019*\"nd\" + 0.016*\"sight\" + '\n",
      "  '0.016*\"signific\" + 0.015*\"separ\" + 0.015*\"god\" + 0.014*\"remedi_sick\" + '\n",
      "  '0.013*\"heal_oil\"')]\n",
      "\n",
      "Perplexity:  -7.740747182684164\n",
      "\n",
      "Coherence Score:  0.4007740383061658\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x7f9b21309220>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA(process_covid_unru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5f4d4e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_objects_json(directory):\n",
    "    json_file = []\n",
    "\n",
    "    for files in os.listdir(directory):\n",
    "        file = os.path.join(directory, files)\n",
    "        json_file.append(file)\n",
    "\n",
    "    #del json_file[0]\n",
    "    event_objects = []\n",
    "\n",
    "    for element in json_file:    \n",
    "        f = open(element,'r')\n",
    "        information = []\n",
    "\n",
    "        for element in f:\n",
    "            content = json.loads(element)\n",
    "            information.append(content)\n",
    "\n",
    "        event_objects.append(information)\n",
    "    \n",
    "    return event_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "06d88549",
   "metadata": {},
   "outputs": [],
   "source": [
    "rumor_json = get_event_objects_json('/Users/dukeisyourdaddy/Desktop/rumour')\n",
    "unrumor_json = get_event_objects_json('/Users/dukeisyourdaddy/Desktop/nonrumour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59be1f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Userfule information:\n",
    "#followers_count, following_count, tweet_count, if descrption is {}, verified\n",
    "#Table 1 - user features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8f76419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#influence - followers\n",
    "#Credibility = verified \n",
    "#User role = followers / friends\n",
    "#Age = creat_id data / publish tweets date\n",
    "#Friends = followees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "77b158de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_features(json):\n",
    "    followers = []\n",
    "    verified = []\n",
    "    user_role = []\n",
    "    age = []\n",
    "    friends = []\n",
    "    \n",
    "    for element in json:\n",
    "        followers.append(element[0]['user']['public_metrics']['followers_count'])\n",
    "        \n",
    "        if element[0]['user']['verified']:\n",
    "            verified.append(1)\n",
    "        else:\n",
    "            verified.append(0)\n",
    "        \n",
    "        if element[0]['user']['public_metrics']['following_count'] != 0:\n",
    "            user_role.append(round(element[0]['user']['public_metrics']['followers_count']/element[0]['user']['public_metrics']['following_count']))\n",
    "        \n",
    "        creat_id_date = element[0]['user']['created_at'][0:10]\n",
    "        publish_date = element[0]['created_at'][0:10]\n",
    "        \n",
    "        def cal_days(day1, day2):\n",
    "            \n",
    "            dt = pd.to_datetime(day1, format='%Y-%m-%d')\n",
    "            dt1 = pd.to_datetime(day2, format='%Y-%m-%d')\n",
    "\n",
    "            return (dt1-dt).days\n",
    "        \n",
    "        age.append(cal_days(creat_id_date,publish_date))\n",
    "        \n",
    "        friends.append(element[0]['user']['public_metrics']['following_count'])\n",
    "\n",
    "    \n",
    "    return followers, verified, user_role, age, friends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a6bb11fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rumor_followers, rumor_verified, rumor_user_role, rumor_age, rumor_friends = get_user_features(rumor_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e69db8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_rumor_followers, un_rumor_verified, un_rumor_user_role, un_rumor_age, un_rumor_friends = get_user_features(unrumor_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "87798db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average(list1):\n",
    "    return sum(list1)/len(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f1ce84c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import median\n",
    "from statistics import variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "346bde84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average----rumor_followers is 4351047.004085481, un_rumor_followers is 5049828.8797527775\n",
      "Average----rumor_user_role is 62076.030006317116, un_rumor_user_role is 88514.44068066157\n",
      "Average----rumor_age is 3405.2536140791954, un_rumor_age is 3358.1394148020654\n",
      "Average----rumor_friends is 11816.711502199874, un_rumor_friends is 6013.3145047723365\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average----rumor_followers is {get_average(rumor_followers)}, un_rumor_followers is {get_average(un_rumor_followers)}\")\n",
    "print(f\"Average----rumor_user_role is {get_average(rumor_user_role)}, un_rumor_user_role is {get_average(un_rumor_user_role)}\")\n",
    "print(f\"Average----rumor_age is {get_average(rumor_age)}, un_rumor_age is {get_average(un_rumor_age)}\")\n",
    "print(f\"Average----rumor_friends is {get_average(rumor_friends)}, un_rumor_friends is {get_average(un_rumor_friends)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ba9d596d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median----rumor_followers is 813918.5, un_rumor_followers is 528566.5\n",
      "Median----rumor_user_role is 361.0, un_rumor_user_role is 359.0\n",
      "Median----rumor_age is 3941.0, un_rumor_age is 3795.5\n",
      "Median----rumor_friends is 1588.0, un_rumor_friends is 1100.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Median----rumor_followers is {median(rumor_followers)}, un_rumor_followers is {median(un_rumor_followers)}\")\n",
    "print(f\"Median----rumor_user_role is {median(rumor_user_role)}, un_rumor_user_role is {median(un_rumor_user_role)}\")\n",
    "print(f\"Median----rumor_age is {median(rumor_age)}, un_rumor_age is {median(un_rumor_age)}\")\n",
    "print(f\"Median----rumor_friends is {median(rumor_friends)}, un_rumor_friends is {median(un_rumor_friends)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c04608b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance----rumor_followers is 112773249180229.73, un_rumor_followers is 159127604235677.75\n",
      "variance----rumor_user_role is 92920854619.22722, un_rumor_user_role is 407152536342.7176\n",
      "variance----rumor_age is 1501080.257885394, un_rumor_age is 1461679.781516534\n",
      "variance----rumor_friends is 1130494753.5200121, un_rumor_friends is 708211753.3152094\n"
     ]
    }
   ],
   "source": [
    "print(f\"variance----rumor_followers is {variance(rumor_followers)}, un_rumor_followers is {variance(un_rumor_followers)}\")\n",
    "print(f\"variance----rumor_user_role is {variance(rumor_user_role)}, un_rumor_user_role is {variance(un_rumor_user_role)}\")\n",
    "print(f\"variance----rumor_age is {variance(rumor_age)}, un_rumor_age is {variance(un_rumor_age)}\")\n",
    "print(f\"variance----rumor_friends is {variance(rumor_friends)}, un_rumor_friends is {variance(un_rumor_friends)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3a1e61e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2529"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rumor_verified.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b8c03714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "653"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rumor_verified.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1c96bab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(653/2529, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f419dda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10121"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "un_rumor_verified.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dcd59559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2661"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "un_rumor_verified.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "70e25d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(2661/10121, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ee6c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
